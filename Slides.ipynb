{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Advanced solar irradiance course part II: Forecasting of solar irradiance\n",
    "\n",
    "### In this part of the course we will continue with statistical learning and give a brief introduction into machine learning. The main difference between the two lies in their purpose: while statistical learning aims to infer the relationship between variables and has the ability to make predictions, machine learning aims to accurately predict while offering less interpretability. \n",
    "\n",
    "### As the following article will show, my explanation was too brief but he makes the distinction very clear! (15 min read)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "%%html\n",
    "<iframe src=\"https://towardsdatascience.com/the-actual-difference-between-statistics-and-machine-learning-64b49f07ea3\" width=\"950\" height=\"500\"></iframe>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Today's outline:\n",
    "\n",
    "### 1) Motivate why we are interested in probabilistic forecasting.\n",
    "\n",
    "### 2) Introduce a few verification tools that we need to assess probabilistic forecasts.\n",
    "\n",
    "### 3) Introduce cross-validation, training and testing in machine learning.\n",
    "\n",
    "### 4) An introduction to regression and some examples.\n",
    "\n",
    "### 5) An introduction machine learning and some examples.\n",
    "\n",
    "### 6) A brief look at data cleaning and data analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## So, let's start with why we are interested in probabilistic forecasting by visualizing some irradiance data from Desert Rock, Nevada. These data are freely available from the National Renewable Energy Laboratory (NREL) through an API, but these are downloaded already."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "dat = pd.read_csv(\"/Users/Dennis/Desktop/SupplementaryMaterials/data/dra_2015-2016.txt\", delimiter = \"\\t\",\n",
    "                 header='infer')\n",
    "dat['Time'] = pd.to_datetime(dat['Time']) # Make sure that pandas recognizes this as datetime\n",
    "dat = dat.set_index('Time') # Set time column as the index (easier for plotting)\n",
    "dat.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Let's plot this to see what we're dealing with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "# Use seaborn style defaults and set the default figure size\n",
    "sns.set(rc={'figure.figsize':(11, 4)})\n",
    "\n",
    "cols_plot = ['dw_solar', 'Ics']\n",
    "axes = dat.loc['2015-01-01 00:00:00':'2016-12-01 00:00:00',cols_plot].plot(alpha=0.75, figsize=(11, 9), subplots=True)\n",
    "for ax in axes:\n",
    "    ax.set_ylabel('Irradiance (W/m^2)')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### We can clearly observe the daily variability in the top figure while we see the seasonal variability in the bottom figure. Let's zoom in a bit further:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "cols_plot = ['dw_solar', 'Ics']\n",
    "axes = dat.loc['2015-04-01 00:00:00':'2015-04-10 00:00:00',cols_plot].plot(alpha=0.75, figsize=(11, 9), subplots=True)\n",
    "for ax in axes:\n",
    "    ax.set_ylabel('Irradiance (W/m^2)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Notice how little variability this time series contains, which is due to its geographical location. Let's see what happens when we load irradiance data from the same period but then from Sioux Falls, South Dakota."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "dat = pd.read_csv(\"/Users/Dennis/Desktop/SupplementaryMaterials/data/sxf_2015-2016.txt\", delimiter = \"\\t\",\n",
    "                 header='infer')\n",
    "dat['Time'] = pd.to_datetime(dat['Time']) # Make sure that pandas recognizes this as datetime\n",
    "dat = dat.set_index('Time') # Set time column as the index (easier for plotting)\n",
    "cols_plot = ['dw_solar', 'Ics']\n",
    "axes = dat.loc['2015-04-01 00:00:00':'2015-04-10 00:00:00',cols_plot].plot(alpha=0.75, figsize=(11, 9), subplots=True)\n",
    "for ax in axes:\n",
    "    ax.set_ylabel('Irradiance (W/m^2)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### This dataset also contains numerical weather prediction (NWP) forecasts, which is an important input when forecasting more than 6 hours into the future (more on that later). Let's first compare these forecasts with the actual irradiance at Sioux Falls again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "cols_plot = ['dw_solar', 'G_nam']\n",
    "axes = dat.loc['2015-04-01 00:00:00':'2015-04-10 00:00:00',cols_plot].plot(alpha=0.75, figsize=(9, 7), subplots=False)\n",
    "axes.set_ylabel('Irradiance (W/m^2)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### In addition to NWP forecasts, this data set also contains the persistence forecasts. The persistence forecast is an important benchmark in solar forecasting because it does not have 'skill'. The reason for this is that the persistence forecast is simply the current observation, e.g., the forecast for one hour or one day ahead is the weather (irradiance/temperature/etc.) I observe now.\n",
    "\n",
    "A plot generally makes it easier to understand:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "cols_plot = ['dw_solar', 'G_nam', 'G_pers']\n",
    "axes = dat.loc['2015-04-01 00:00:00':'2015-04-10 00:00:00',cols_plot].plot(alpha=0.75, figsize=(9, 7), subplots=False)\n",
    "axes.set_ylabel('Irradiance (W/m^2)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### The strategy of plotting the data as a time series is a bit cumbersome, more effective would be to plot a scatterplot of the forecasts versus the observations. This reveals all time-independent information of the forecasts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from pandas.plotting import scatter_matrix\n",
    "scatter_matrix(dat[['dw_solar','G_nam', 'G_pers']], alpha=0.5, figsize=(8, 8), diagonal='kde')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### The former plot reveals a systematic positive bias of the NWP forecast model, i.e.,  it tends to overestimate the irradiance. \n",
    "This is quite a common feature of NWP models and an entire field is dedicated to solving these issue (called postprocessing). However, by mapping irradiance to PV power through a non-linear function, e.g., a machine learning model, we implicitly postprocess the NWP forecast.\n",
    "### The main point of these plots was to make the case for probabilistic forecasting, since future irradiance is a random variable and a forecast of the mean gives an incomplete picture of future irradiance. Furthermore, there are uncertainties in the data we have observed (e.g., sensor noise), the model that we have selected (are the model assumptions realistic enough?) and the model parameters that we have learned from our training data.\n",
    "Additionally, it allows us to improve our decision making, e.g., should I bring an umbrella when there's only a 15% chance of rain or how can I optimally schedule EV charging given a number of irradiance scenarios? Obviously, there are many more use cases, especially related to (optimal) control.\n",
    "\n",
    "## Some notation:\n",
    "\n",
    "Let $y_i$ be the observed data during period $i$, $i=1,\\ldots,N$, i.e., $\\mathbf{y} = \\left(y_1, y_2, \\ldots, y_N\\right)^T$. There is some function $f$ that maps $x_i$ to $y_i$ as: $y_i = f(x_i) + \\epsilon_i$, where $\\epsilon_i$ is i.i.d. $\\mathcal{N}(0,\\sigma^2)$. In words this means that there exists a (non-)linear function $f$ that allows us to directly relate $x$ to $y$ while removing noise from the true signal. It is the function $f$ that we try to learn from the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Data organization in a machine-learning setting:\n",
    "\n",
    "In order to be able to learn $f$, we first need to organize our data set such that we have an $n \\times p$ training matrix containing $n$ observations of $p$ input variables / features / dependent variables. This needs to be combined with an observation vector of length $n$. This is formulated as follows:\n",
    "\n",
    "$\\textbf{X}_{\\text{train}}$ = \n",
    "\\begin{bmatrix}\n",
    "    x_{1,1} & x_{1,2} & x_{1,3} & \\dots  & x_{1,p} \\\\\n",
    "    x_{2,1} & x_{2,2} & x_{2,3} & \\dots  & x_{2,p} \\\\\n",
    "    \\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "    x_{n,1} & x_{n,2} & x_{n,3} & \\dots  & x_{n,p}\n",
    "\\end{bmatrix}\n",
    "\n",
    "$\\textbf{y}_{\\text{train}} = \\left(y_1, y_2, \\ldots, y_n\\right)^T$\n",
    "\n",
    "$\\textbf{y}_{\\text{train}} = \\textbf{X}_{\\text{train}}$\n",
    "\n",
    "#### The simplest model would be a line, which is exactly what linear regression is. \n",
    "The observation vector $\\textbf{y}_{\\text{train}}$ and feature matrix $\\textbf{X}_{\\text{train}}$ are then related through parameters $\\mathbf{\\beta} = \\left(\\beta_1, \\beta_2, \\ldots, \\beta_p\\right)$ as follows: \n",
    "$\\textbf{y}_{\\text{train}} = \\textbf{X}_{\\text{train}} \\cdot \\mathbf{\\beta}$.\n",
    "We can find the $p+1$ parameters of the linear regression model by minimizing the sum of squared errors (SSE):\n",
    "$L(\\beta) = \\sum^n_{i=1} | y_i - \\sum_{j=1}^p X_{ij}\\beta_j |^2$ = $\\| \\textbf{y}_{\\text{train}} - \\textbf{X}_{\\text{train}} \\cdot \\mathbf{\\beta} \\|^2$. We have now estimated the parameters to compute the $\\textit{conditional mean}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Once the model has been identified and the parameters have been learned, we can use our function on a test set that we held out from the training set to make predictions:\n",
    "\n",
    "$\\textbf{X}_{\\text{test}}$ = \n",
    "\\begin{bmatrix}\n",
    "    x_{n+1,1} & x_{n+1,2} & x_{n+1,3} & \\dots  & x_{n+1,p} \\\\\n",
    "    x_{n+2,1} & x_{n+2,2} & x_{n+2,3} & \\dots  & x_{n+2,p} \\\\\n",
    "    \\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "    x_{N,1} & x_{N,2} & x_{N,3} & \\dots  & x_{N,p}\n",
    "\\end{bmatrix}\n",
    "\n",
    "$\\mathbf{\\hat{y}}_{\\text{pred}} = \\textbf{X}_{\\text{test}} \\cdot \\hat{\\mathbf{\\beta}} = \\left(\\hat{y}_{n+1}, \\hat{y}_{n+2}, \\ldots, \\hat{y}_N\\right)^T$\n",
    "\n",
    "### Error metrics:\n",
    "The sum of squared errors (SSE) is closely related to one of the most common error metrics: the root mean squared error. It has many advantages such as that it penalizes outliers more severely (which is what you'd want in forecasting) and that is has the same unit as the dependent variable. It is defined as RMSE = $\\sqrt{\\frac{1}{N-n+1}\\sum^N_{i=n+1} (y_i - \\hat{y}_i)^2}$. Another common metric is the mean absolute error (MAE): $\\frac{1}{N-n+1}\\sum^N_{i=n+1} |y_i - \\hat{y}_i|$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Now that we formally have defined the training and test sets and two deterministic error metrics, we can introduce cross-validation (CV). \n",
    "Regression and machine learning models have the tendency to \"overfit\" to the training data when our sole objective is to minimize some loss function, as we will now see using linear regression. In order to achieve that, we generate some random data and generate a cubic function from that with additional noise. Then, we fit a linear function to the data and compare this to when we fit a function that is linear in the parameters but non-linear in the input variables (and therefore still linear regression):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Code adapted from: https://towardsdatascience.com/polynomial-regression-bbe8b9d97491\n",
    "\n",
    "import operator\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "np.random.seed(123)\n",
    "x = 1 - 2 * np.random.normal(0, 1, 15)\n",
    "y = x - 3 * (x ** 2) + 0.25 * (x ** 3) + np.random.normal(-2, 3.5, 15)\n",
    "\n",
    "# transforming the data to include another axis\n",
    "x = x[:, np.newaxis]\n",
    "\n",
    "polynomial_features= PolynomialFeatures(degree=1)\n",
    "x_poly = polynomial_features.fit_transform(x)\n",
    "\n",
    "model_lr = LinearRegression()\n",
    "model_lr.fit(x_poly, y)\n",
    "y_poly_pred = model_lr.predict(x_poly)\n",
    "\n",
    "rmse = np.sqrt(mean_squared_error(y,y_poly_pred))\n",
    "r2 = r2_score(y,y_poly_pred)\n",
    "print(\"RMSE =\",rmse)\n",
    "print(\"$R^2$ =\",r2)\n",
    "\n",
    "plt.scatter(x, y, s=10)\n",
    "# sort the values of x before line plot\n",
    "sort_axis = operator.itemgetter(0)\n",
    "sorted_zip = sorted(zip(x,y_poly_pred), key=sort_axis)\n",
    "x, y_poly_pred = zip(*sorted_zip)\n",
    "plt.plot(x, y_poly_pred, color='m', label='df = 1')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from ipywidgets import interactive\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def f(df):\n",
    "    np.random.seed(123)\n",
    "    x = 1 - 2 * np.random.normal(0, 1, 15)\n",
    "    y = x - 3 * (x ** 2) + 0.25 * (x ** 3) + np.random.normal(-2, 3.5, 15)\n",
    "\n",
    "    # transforming the data to include another axis\n",
    "    x = x[:, np.newaxis]\n",
    "\n",
    "    polynomial_features= PolynomialFeatures(degree=df)\n",
    "    x_poly = polynomial_features.fit_transform(x)\n",
    "\n",
    "    model_lr = LinearRegression()\n",
    "    model_lr.fit(x_poly, y)\n",
    "    y_poly_pred = model_lrmodel.predict(x_poly)\n",
    "    \n",
    "    rmse = np.sqrt(mean_squared_error(y,y_poly_pred))\n",
    "    r2 = r2_score(y,y_poly_pred)\n",
    "    print(\"RMSE =\",rmse)\n",
    "    print(\"R^2 =\",r2)\n",
    "    \n",
    "    plt.scatter(x, y, s=10)\n",
    "    # sort the values of x before line plot\n",
    "    sort_axis = operator.itemgetter(0)\n",
    "    sorted_zip = sorted(zip(x,y_poly_pred), key=sort_axis)\n",
    "    x, y_poly_pred = zip(*sorted_zip)\n",
    "    plt.plot(x, y_poly_pred, color='m', label=('df = '+str(df)))\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "interactive_plot = interactive(f, df=(1, 15))\n",
    "output = interactive_plot.children[-1]\n",
    "output.layout.height = '350px'\n",
    "interactive_plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### As we can see, we can increase the number of degrees of our polynomial to improve the fit to the training. However, this does not mean that our model performs better on unseen data. In fact, it is quite the opposite, which is why we need to train on multiple slices of our training data and validate (test) how the model \"generalizes\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%html\n",
    "<iframe src=\"https://en.wikipedia.org/wiki/Cross-validation_(statistics)#Purpose_of_cross-validation\" width=\"950\" height=\"500\"></iframe>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Due to the autocorrelation in time series data, K-fold CV is not appropriate. \n",
    "Luckily, scikit offers TimeSeriesSplit (R has a similar feature in the $\\texttt{caret}$ package). Below is a visualization of the way the time series is split up when performing cross-validation. Code from: https://scikit-learn.org/stable/auto_examples/model_selection/plot_cv_indices.html#sphx-glr-auto-examples-model-selection-plot-cv-indices-py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Visualizing cross-validation behavior in scikit-learn\n",
    "=====================================================\n",
    "\n",
    "Choosing the right cross-validation object is a crucial part of fitting a\n",
    "model properly. There are many ways to split data into training and test\n",
    "sets in order to avoid model overfitting, to standardize the number of\n",
    "groups in test sets, etc.\n",
    "\n",
    "This example visualizes the behavior of several common scikit-learn objects\n",
    "for comparison.\n",
    "\"\"\"\n",
    "\n",
    "from sklearn.model_selection import (TimeSeriesSplit, KFold, ShuffleSplit,\n",
    "                                     StratifiedKFold, GroupShuffleSplit,\n",
    "                                     GroupKFold, StratifiedShuffleSplit)\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Patch\n",
    "np.random.seed(1338)\n",
    "cmap_data = plt.cm.Paired\n",
    "cmap_cv = plt.cm.coolwarm\n",
    "n_splits = 4\n",
    "\n",
    "###############################################################################\n",
    "# Visualize our data\n",
    "# ------------------\n",
    "#\n",
    "# First, we must understand the structure of our data. It has 100 randomly\n",
    "# generated input datapoints, 3 classes split unevenly across datapoints,\n",
    "# and 10 \"groups\" split evenly across datapoints.\n",
    "#\n",
    "# As we'll see, some cross-validation objects do specific things with\n",
    "# labeled data, others behave differently with grouped data, and others\n",
    "# do not use this information.\n",
    "#\n",
    "# To begin, we'll visualize our data.\n",
    "\n",
    "# Generate the class/group data\n",
    "n_points = 100\n",
    "X = np.random.randn(100, 10)\n",
    "\n",
    "percentiles_classes = [.1, .3, .6]\n",
    "y = np.hstack([[ii] * int(100 * perc)\n",
    "               for ii, perc in enumerate(percentiles_classes)])\n",
    "\n",
    "# Evenly spaced groups repeated once\n",
    "groups = np.hstack([[ii] * 10 for ii in range(10)])\n",
    "\n",
    "\n",
    "def visualize_groups(classes, groups, name):\n",
    "    # Visualize dataset groups\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.scatter(range(len(groups)),  [.5] * len(groups), c=groups, marker='_',\n",
    "               lw=50, cmap=cmap_data)\n",
    "    ax.scatter(range(len(groups)),  [3.5] * len(groups), c=classes, marker='_',\n",
    "               lw=50, cmap=cmap_data)\n",
    "    ax.set(ylim=[-1, 5], yticks=[.5, 3.5],\n",
    "           yticklabels=['Data\\ngroup', 'Data\\nclass'], xlabel=\"Sample index\")\n",
    "\n",
    "\n",
    "visualize_groups(y, groups, 'no groups')\n",
    "\n",
    "###############################################################################\n",
    "# Define a function to visualize cross-validation behavior\n",
    "# --------------------------------------------------------\n",
    "#\n",
    "# We'll define a function that lets us visualize the behavior of each\n",
    "# cross-validation object. We'll perform 4 splits of the data. On each\n",
    "# split, we'll visualize the indices chosen for the training set\n",
    "# (in blue) and the test set (in red).\n",
    "\n",
    "\n",
    "def plot_cv_indices(cv, X, y, group, ax, n_splits, lw=10):\n",
    "    \"\"\"Create a sample plot for indices of a cross-validation object.\"\"\"\n",
    "\n",
    "    # Generate the training/testing visualizations for each CV split\n",
    "    for ii, (tr, tt) in enumerate(cv.split(X=X, y=y, groups=group)):\n",
    "        # Fill in indices with the training/test groups\n",
    "        indices = np.array([np.nan] * len(X))\n",
    "        indices[tt] = 1\n",
    "        indices[tr] = 0\n",
    "\n",
    "        # Visualize the results\n",
    "        ax.scatter(range(len(indices)), [ii + .5] * len(indices),\n",
    "                   c=indices, marker='_', lw=lw, cmap=cmap_cv,\n",
    "                   vmin=-.2, vmax=1.2)\n",
    "\n",
    "    # Plot the data classes and groups at the end\n",
    "    ax.scatter(range(len(X)), [ii + 1.5] * len(X),\n",
    "               c=y, marker='_', lw=lw, cmap=cmap_data)\n",
    "\n",
    "    ax.scatter(range(len(X)), [ii + 2.5] * len(X),\n",
    "               c=group, marker='_', lw=lw, cmap=cmap_data)\n",
    "\n",
    "    # Formatting\n",
    "    yticklabels = list(range(n_splits)) + ['class', 'group']\n",
    "    ax.set(yticks=np.arange(n_splits+2) + .5, yticklabels=yticklabels,\n",
    "           xlabel='Sample index', ylabel=\"CV iteration\",\n",
    "           ylim=[n_splits+2.2, -.2], xlim=[0, 100])\n",
    "    ax.set_title('{}'.format(type(cv).__name__), fontsize=15)\n",
    "    return ax\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "# Let's see how it looks for the `KFold` cross-validation object:\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "cv = KFold(n_splits)\n",
    "plot_cv_indices(cv, X, y, groups, ax, n_splits)\n",
    "\n",
    "###############################################################################\n",
    "# As you can see, by default the KFold cross-validation iterator does not\n",
    "# take either datapoint class or group into consideration. We can change this\n",
    "# by using the ``StratifiedKFold`` like so.\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "cv = StratifiedKFold(n_splits)\n",
    "plot_cv_indices(cv, X, y, groups, ax, n_splits)\n",
    "\n",
    "###############################################################################\n",
    "# In this case, the cross-validation retained the same ratio of classes across\n",
    "# each CV split. Next we'll visualize this behavior for a number of CV\n",
    "# iterators.\n",
    "#\n",
    "# Visualize cross-validation indices for many CV objects\n",
    "# ------------------------------------------------------\n",
    "#\n",
    "# Let's visually compare the cross validation behavior for many\n",
    "# scikit-learn cross-validation objects. Below we will loop through several\n",
    "# common cross-validation objects, visualizing the behavior of each.\n",
    "#\n",
    "# Note how some use the group/class information while others do not.\n",
    "\n",
    "cvs = [KFold, GroupKFold, ShuffleSplit, StratifiedKFold,\n",
    "       GroupShuffleSplit, StratifiedShuffleSplit, TimeSeriesSplit]\n",
    "\n",
    "\n",
    "for cv in cvs:\n",
    "    this_cv = cv(n_splits=n_splits)\n",
    "    fig, ax = plt.subplots(figsize=(6, 3))\n",
    "    plot_cv_indices(this_cv, X, y, groups, ax, n_splits)\n",
    "\n",
    "    ax.legend([Patch(color=cmap_cv(.8)), Patch(color=cmap_cv(.02))],\n",
    "              ['Testing set', 'Training set'], loc=(1.02, .8))\n",
    "    # Make the legend fit\n",
    "    plt.tight_layout()\n",
    "    fig.subplots_adjust(right=.7)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### There are other ways to prevent overfitting. A common approach is regularization, which adds a penalty (regularization) term to the loss function. Common regularizers are the least absolute shrinkage and selection operator (LASSO) and ridge regression. We will not go into these methods but below is a link to the Wikipedia page on the LASSO."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%html\n",
    "<iframe src=\"https://en.wikipedia.org/wiki/Lasso_(statistics)\" width=\"950\" height=\"500\"></iframe>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now that we have introduced all necessary aspects, we can combine these and try to produce our first forecast.\n",
    "Let's start with the simplest model (linear regression, which we have seen earlier) and apply it to the irradiance data of Sioux Falls, South Dakota."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "\n",
    "dat = pd.read_csv(\"/Users/Dennis/Desktop/SupplementaryMaterials/data/sxf_2015-2016.txt\", delimiter = \"\\t\",\n",
    "                 header='infer')\n",
    "dat['Time'] = pd.to_datetime(dat['Time']) # Make sure that pandas recognizes this as datetime\n",
    "dat = dat.set_index('Time') # Set time column as the index (easier for plotting)\n",
    "dat = dat.dropna(axis=0) # Remove the NaNs\n",
    "\n",
    "# Define the training and test set:\n",
    "X_train = dat.loc['2015-01-01':'2016-01-31', 'Ics':'G_nam']\n",
    "y_train = dat.loc['2015-01-01':'2016-01-31', 'dw_solar']\n",
    "\n",
    "X_test = dat.loc['2016-02-01':'2016-12-31', 'Ics':'G_nam']\n",
    "y_test = dat.loc['2016-02-01':'2016-12-31', 'dw_solar']\n",
    "\n",
    "model_lr = LinearRegression().fit(X_train,y_train)\n",
    "y_hat = model_lr.predict(X_test)\n",
    "\n",
    "idx = y_test.to_numpy().nonzero()[0] # Check the non-zero values and disregard the rest\n",
    "rmse = np.sqrt(mean_squared_error(y_test[idx],y_hat[idx]))\n",
    "mae = mean_absolute_error(y_test[idx],y_hat[idx])\n",
    "r2 = r2_score(y_test[idx],y_hat[idx])\n",
    "\n",
    "print('MAE =',mae)\n",
    "print('RMSE =',rmse)\n",
    "print('r2 =',r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare this to the NWP forecast:\n",
    "idx = y_test.to_numpy().nonzero()[0] # Check the non-zero values and disregard the rest\n",
    "y_test = dat.loc['2016-02-01':'2016-12-31', 'G_nam']\n",
    "rmse = np.sqrt(mean_squared_error(y_test[idx],y_hat[idx]))\n",
    "mae = mean_absolute_error(y_test[idx],y_hat[idx])\n",
    "r2 = r2_score(y_test[idx],y_hat[idx])\n",
    "print('MAE =',mae)\n",
    "print('RMSE =',rmse)\n",
    "print('r2 =',r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "y_hat = pd.Series(y_hat, index = y_test.index)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(y_test['2016-02-01':'2016-02-06'], label = \"Observations\")\n",
    "ax.plot(y_hat['2016-02-01':'2016-02-06'], label = \"Forecasts\")\n",
    "\n",
    "ax.set(xlabel='Time', ylabel='Irradiance (W/m^2)')\n",
    "ax.legend()\n",
    "ax.figsize=(11, 9)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Not bad, but now we want to address the matter of uncertainty. For that, we first need to look into quantile regression.\n",
    "Instead of minimizing the sum of squared errors to find the conditional mean, we minimize a tilted loss function to find certain $\\textit{quantiles}$. First, let's see what a quantile actually is on Wikipedia:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%html\n",
    "<iframe src=\"https://en.wikipedia.org/wiki/Quantile\" width=\"950\" height=\"500\"></iframe>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%html\n",
    "<iframe src=\"https://en.wikipedia.org/wiki/Quantile_function\" width=\"950\" height=\"500\"></iframe>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### As mentioned above, in quantile regression we estimate a tilted loss function that depends on the quantile we try to estimate. As always, it's easier to understand with an image.\n",
    "Basically, the tilted loss function penalizes higher quantiles more for negative errors and lower quantiles more for positive errors. This loss function is also called the pinball score because it can also be used as a score to assess the quality of a probabilistic forecast (similar to RMSE and MAE for deterministic forecasts)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def quantile_loss(q, y, f):\n",
    "  # q: Quantile to be evaluated, e.g., 0.5 for median.\n",
    "  # y: True value.\n",
    "  # f: Forecast value.\n",
    "  e = y - f\n",
    "  return np.maximum(q * e, (q - 1) * e)\n",
    "\n",
    "y_hat = np.arange(start=-1,stop=1,step=0.1)\n",
    "y_test = np.zeros(len(y_hat))\n",
    "error = y_hat - y_test\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(error, quantile_loss(0.2, y_test, y_hat), label = \"q = 0.2\")\n",
    "ax.plot(error, quantile_loss(0.5, y_test, y_hat), label = \"q = 0.5\")\n",
    "ax.plot(error, quantile_loss(0.8, y_test, y_hat), label = \"q = 0.8\")\n",
    "ax.set(xlabel=\"error\",ylabel=\"quantile loss\")\n",
    "fig.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Another important score is the continuous ranked probability score (CRPS). Its definition is such that it generalizes to the MAE if we replace the probabilistic forecast with a deterministic one.\n",
    "Like the pinball score, it is a so-called 'proper' score. In short, this is to say that the score has the property that truthtelling is the most rewarding strategy. The definition and a figure can be found in my licentiate thesis (p. 31):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%html\n",
    "<iframe src=\"http://www.diva-portal.org/smash/get/diva2:1256832/FULLTEXT01.pdf\" width=\"950\" height=\"750\"></iframe>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now that we have seen the core of quantile regression (QR), let's apply it to the data we applied linear regression to and evaluate what we get."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm \n",
    "taus = np.arange(0.1,0.91,0.1) # Define the quantiles we wish to train and forecast\n",
    "# Xtra_jitter = np.random.normal(1*Xtra,0.01) # Add some random noise to avoid singular matrix\n",
    "quantreg = sm.QuantReg(y_train, X_train)  \n",
    "y_hat_qr = np.vstack( [quantreg.fit(q=q, max_iter=10000).predict(X_test) for q in taus], ).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CRPS: (no need to understand this code)\n",
    "import warnings\n",
    "import properscoring as ps\n",
    "def CRPS(ens,obs):\n",
    "\n",
    "    if type(obs) is pd.Series:\n",
    "        obs = obs.to_numpy()\n",
    "    if type(ens) is pd.DataFrame:\n",
    "        ens = ens.to_numpy()\n",
    "    obs = np.reshape(obs, (len(obs),1))\n",
    "    idx = np.c_[obs,ens]\n",
    "    idx = ~np.isnan(idx).any(axis=1) # check rows with NaNs\n",
    "    ens = ens[idx,:] # remove rows with NaNs\n",
    "    obs = obs[idx]\n",
    "    ens.sort(axis=1) # Make sure there's no quantile crossing\n",
    "    \n",
    "    lst = []\n",
    "    for k in range(ens.shape[0]):\n",
    "        lst.append(ps.crps_ensemble(obs[k,0], ens[k]))\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\", category=RuntimeWarning)\n",
    "        return lst,np.nanmean(lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = dat.loc['2016-02-01':'2016-12-31', 'dw_solar']\n",
    "idx = y_test.to_numpy().nonzero()[0] # Check the non-zero values and disregard the rest\n",
    "\n",
    "print(\"CRPS =\",CRPS(y_hat_qr[idx,:],y_test[idx])[1])\n",
    "print(\"Quantile score =\",np.mean([quantile_loss(q,y_test[idx],y_hat_qr[idx,f]) for q,f in zip(taus,range(9))]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ens = pd.DataFrame(y_hat_qr, index=y_test.index)\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(ens.loc['2016-02-10':'2016-02-16',0], label = \"10% quantile\")\n",
    "ax.plot(ens.loc['2016-02-10':'2016-02-16',8], label = \"90% quantile\")\n",
    "ax.plot(y_test['2016-02-10':'2016-02-16'], label = \"Observations\")\n",
    "\n",
    "ax.set(xlabel='Time', ylabel='Irradiance (W/m^2)')\n",
    "ax.legend()\n",
    "ax.figsize=(15, 13)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### It seems that the forecasts are better than those of the linear regression model (remember: CRPS allows for comparison with MAE of deterministic forecasts).\n",
    "Unfortunately, numerical scores only tell part of the story and we therefore have to look at the distribution of our forecasts to see if they are well $\\textit{calibrated}$. We can do this using the reliability diagram and the rank histogram. These visual verification tools allow us to determine whether the forecast distribution is, on average, indistinguishable from the distribution of the observations. Let's look at some examples with simulated data first and then continue with our forecast results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define some functions that we'll use for forecast verification. There\n",
    "# is no need to understand these.\n",
    "\n",
    "from scipy.stats import binom\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.distributions.empirical_distribution import ECDF\n",
    "import numpy as np\n",
    "from statistics import mean\n",
    "import properscoring as ps\n",
    "import pandas as pd\n",
    "import warnings\n",
    "\n",
    "def quantile_loss(q, y, f):\n",
    "  # q: Quantile to be evaluated, e.g., 0.5 for median.\n",
    "  # y: True value.\n",
    "  # f: Fitted (predicted) value.\n",
    "  e = y - f\n",
    "  return np.maximum(q * e, (q - 1) * e)\n",
    "\n",
    "def interval_score(l, u, y, alpha):\n",
    "    lst = []\n",
    "    for i in range(len(y)):\n",
    "        if y[i] < l[i]:\n",
    "           lst.append((u[i] - l[i]) + 2/alpha * (l[i] - y[i]))\n",
    "        elif y[i] > u[i]:\n",
    "             lst.append((u[i] - l[i]) + 2/alpha * (y[i] - u[i]))\n",
    "        else:\n",
    "             lst.append((u[i] - l[i]))\n",
    "    return lst\n",
    "\n",
    "# Reliability diagram:\n",
    "def reliabilityDiagram(ens,obs):\n",
    "    if type(obs) is pd.Series:\n",
    "        obs = obs.to_numpy()\n",
    "    if type(ens) is pd.DataFrame:\n",
    "        ens = ens.to_numpy()\n",
    "    obs = np.reshape(obs, (len(obs),1))\n",
    "    idx = np.c_[obs,ens]\n",
    "    idx = ~np.isnan(idx).any(axis=1) # check rows with NaNs\n",
    "    ens = ens[idx,:] # remove rows with NaNs\n",
    "    obs = obs[idx]\n",
    "    ens.sort(axis=1) # Make sure there's no quantile crossing\n",
    "    \n",
    "    taus = np.arange(0,1,(1/(ens.shape[1]+1)))\n",
    "    taus = taus[1:len(taus)]\n",
    "    res = np.zeros((ens.shape[1],4))\n",
    "    res[:,3] = taus\n",
    "    \n",
    "    # Reliability:\n",
    "    M_obs = np.repeat(obs,ens.shape[1],axis=1)\n",
    "    res[:,0] = np.sum(M_obs < ens, axis = 0)/ens.shape[0]\n",
    "     \n",
    "    # Consistency bars:\n",
    "    q_max = np.repeat(max(taus),ens.shape[1])\n",
    "    q_min = np.repeat(min(taus),ens.shape[1])\n",
    "    n = np.repeat(ens.shape[0], ens.shape[1])\n",
    "    p = taus\n",
    "    res[:,1] = binom.ppf(q_max,n,p)/ens.shape[0]\n",
    "    res[:,2] = binom.ppf(q_min,n,p)/ens.shape[0]\n",
    "        \n",
    "    return res\n",
    "\n",
    "def myPlot(reliabilityArray):\n",
    "    \n",
    "    fig = plt.figure()\n",
    "    plt.plot(reliabilityArray[:,3], reliabilityArray[:,0], 'r', label = 'Reliability')\n",
    "    plt.plot(reliabilityArray[:,3], reliabilityArray[:,3], 'k--', label = 'Ideal')\n",
    "    plt.plot(reliabilityArray[:,3], reliabilityArray[:,1], 'grey', label = 'Upper bound')\n",
    "    plt.plot(reliabilityArray[:,3], reliabilityArray[:,2], 'grey', label = 'Lower bound')\n",
    "    plt.legend()\n",
    "    return(fig)\n",
    "    \n",
    "    # Rank histogram:\n",
    "def rankHistogram(ens,obs):\n",
    "    if type(obs) is pd.Series:\n",
    "        obs = obs.to_numpy()\n",
    "    if type(ens) is pd.DataFrame:\n",
    "        ens = ens.to_numpy()\n",
    "    obs = np.reshape(obs, (len(obs),1))\n",
    "    idx = np.c_[obs,ens]\n",
    "    idx = ~np.isnan(idx).any(axis=1) # check rows with NaNs\n",
    "    ens = ens[idx,:] # remove rows with NaNs\n",
    "    obs = obs[idx]\n",
    "    ens.sort(axis=1) # Make sure there's no quantile crossing\n",
    "    \n",
    "    z = np.zeros((ens.shape[0],1))\n",
    "    ecdf = np.apply_along_axis(ECDF, 1, ens)\n",
    "    for i in range(ens.shape[0]):\n",
    "        z[i] = ecdf[i](obs[i])\n",
    "    \n",
    "    taus = np.arange(0,1,(1/(ens.shape[1]+1)))\n",
    "    taus = taus[1:len(taus)]\n",
    "\n",
    "    up = binom.ppf(max(taus), ens.shape[0], 1/(ens.shape[1]+1)) / ens.shape[0]\n",
    "    down = binom.ppf(min(taus), ens.shape[0], 1/(ens.shape[1]+1)) / ens.shape[0]\n",
    "\n",
    "    plt.hist(z, weights=np.ones(len(z)) / len(z))\n",
    "    plt.hlines(1/(ens.shape[1]+1),colors='red',xmin=0,xmax=1,linestyles='--')\n",
    "    plt.hlines(up,xmin=0,xmax=1,linestyles=':')\n",
    "    plt.hlines(down,xmin=0,xmax=1,linestyles=':')\n",
    "    \n",
    "    # PINAW:\n",
    "def PINAW(ens,obs,plot=True):\n",
    "    if type(obs) is pd.Series:\n",
    "        obs = obs.to_numpy()\n",
    "    if type(ens) is pd.DataFrame:\n",
    "        ens = ens.to_numpy()\n",
    "    obs = np.reshape(obs, (len(obs),1))\n",
    "    idx = np.c_[obs,ens]\n",
    "    idx = ~np.isnan(idx).any(axis=1) # check rows with NaNs\n",
    "    ens = ens[idx,:] # remove rows with NaNs\n",
    "    obs = obs[idx]\n",
    "    ens.sort(axis=1) # Make sure there's no quantile crossing    idx = ~np.isnan(idx).any(axis=1) # check rows with NaNs\n",
    "            \n",
    "    R = max(obs) - min(obs)\n",
    "    nrows = int(np.floor(ens.shape[1]/2))\n",
    "    PINAW = np.zeros(nrows)\n",
    "    taus = np.arange(0,1,(1/(len(PINAW)+1)))\n",
    "    taus = taus[1:len(taus)]\n",
    "    taus = np.flip(taus)\n",
    "    \n",
    "    for c in range(nrows):\n",
    "        PINAW[c] = (1/R) * mean(ens[:,(ens.shape[1]-1-c)] - ens[:,c])\n",
    "        \n",
    "    if plot == True:\n",
    "        return plt.plot(taus,PINAW)\n",
    "    elif plot == False:\n",
    "        return pd.DataFrame({'tau':taus, 'PINAW':PINAW})\n",
    "    \n",
    "    # PICP:\n",
    "def PICP(ens,obs):\n",
    "\n",
    "    if type(obs) is pd.Series:\n",
    "        obs = obs.to_numpy()\n",
    "    if type(ens) is pd.DataFrame:\n",
    "        ens = ens.to_numpy()\n",
    "    obs = np.reshape(obs, (len(obs),1))\n",
    "    idx = np.c_[obs,ens]\n",
    "    idx = ~np.isnan(idx).any(axis=1) # check rows with NaNs\n",
    "    ens = ens[idx,:] # remove rows with NaNs\n",
    "    obs = obs[idx]\n",
    "    ens.sort(axis=1) # Make sure there's no quantile crossing\n",
    "    \n",
    "    nrows = int(np.floor(ens.shape[1]/2))\n",
    "    PICP = np.zeros((nrows,1))\n",
    "    taus = np.arange(0,1,(1/(len(PICP)+1)))\n",
    "    taus = taus[1:len(taus)]\n",
    "    taus = np.flip(taus)\n",
    "    \n",
    "    for c in range(nrows):\n",
    "        PICP[c,] = sum(sum((ens[:,c]<=obs.T) & (ens[:,(ens.shape[1]-1-c)] >= obs.T))) / ens.shape[0]\n",
    "    \n",
    "    plt.plot(taus,PICP, 'red', label = 'PICP')\n",
    "    plt.plot(taus,taus, 'k--', label = 'Ideal')\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate random data from standard normal distribution:\n",
    "ens = np.random.normal(0,1,size=(100,9)) # Random ensemble forecast vector\n",
    "obs = np.random.normal(0,1,size=(100,1)) # Random observation vector\n",
    "myPlot(reliabilityDiagram(ens,obs))\n",
    "rankHistogram(ens,obs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is interesting to note that it is still possible we observe non-reliable forecasts, even though the observations and forecasts come from the same distribution. This is due to the randomness in data sets of limited length and is why we have drawn upper and lower bounds. Regardless, it can still happen that the forecasts are considered unreliable and it is therefore important to have a test set of considerable length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate random data from standard normal distribution with biased forecast:\n",
    "ens = np.random.normal(0.75,1,size=(100,9)) # Random ensemble forecast vector\n",
    "obs = np.random.normal(0,1,size=(100,1)) # Random observation vector\n",
    "myPlot(reliabilityDiagram(ens,obs))\n",
    "rankHistogram(ens,obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate random data from standard normal distribution with unbiased but overdispersed forecast:\n",
    "ens = np.random.normal(0,2,size=(100,9)) # Random ensemble forecast vector\n",
    "obs = np.random.normal(0,1,size=(100,1)) # Random observation vector\n",
    "myPlot(reliabilityDiagram(ens,obs))\n",
    "rankHistogram(ens,obs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### These were some examples of what type of deficiencies we might encounter, which we could not have identified by simply looking at the CRPS or quantile score.\n",
    "Let's continue our visual inspection with the probabilistic forecasts on the irradiance data meauserd in Sioux Falls, South Dakota."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myPlot(reliabilityDiagram(y_hat_qr[idx,:],y_test[idx]))\n",
    "rankHistogram(y_hat_qr[idx,:],y_test[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show sharpness in normal distribution.\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "import math\n",
    "\n",
    "mu = 0\n",
    "x = np.linspace(-3,3,100)\n",
    "for i in np.arange(0.25,2,0.5):\n",
    "    plt.plot(x, stats.norm.pdf(x, mu, i))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from ipywidgets import interactive\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def f(std):\n",
    "    mu = 0\n",
    "    x = np.linspace(-3,3,100)\n",
    "    plt.plot(x, stats.norm.pdf(x, mu, std))\n",
    "    plt.show()\n",
    "\n",
    "interactive_plot = interactive(f, std=np.arange(0.25,2,0.25))\n",
    "output = interactive_plot.children[-1]\n",
    "output.layout.height = '350px'\n",
    "interactive_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PINAW(y_hat_qr[idx,:],y_test[idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Even though we produce sharp probabilistic forecasts, we still underestimate the irradiance by quite a lot, which makes sense in a way, because we're only relying on biased NWP inputs. The quantile regression model is unable to properly correct the bias.\n",
    "Let's see if we can further improve these forecasts by applying a non-linear model such as quantile regression forests (QRFs). For that, we need a brief introduction to decision trees and random forests. A decision tree is a very intuitive predictive model that has low bias but high variance. The cool thing about it is that it is pure arithmetic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%html\n",
    "<iframe src=\"https://en.wikipedia.org/wiki/Decision_tree\" width=\"950\" height=\"500\"></iframe>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%html\n",
    "<iframe src=\"https://en.wikipedia.org/wiki/Random_forest\" width=\"950\" height=\"500\"></iframe>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The main difference between random forests and quantile regression forests is that instead of outputting the mean of the observations in an end node, it outputs the cumulative distribution function (CDF).\n",
    "Scikit-learn has an implementation of the random forest algorithm, while Scikit-garden offers the quantile regression forest algorithm. Even though the model is quite robust in terms of hyperparameter selection, it's still a good exercise to perform cross-validation and find the optimal (in terms of, e.g., CRPS) hyperparameters. In order to do that efficiently, we need to define a few functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declare the quantile regression forest model with its hyperparameters\n",
    "from skgarden import RandomForestQuantileRegressor\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "import itertools \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import properscoring as ps\n",
    "\n",
    "def qrf(params):\n",
    "    min_samples_split, min_samples_leaf, max_features, max_depth, n_estimators = params\n",
    "    n_estimators = int(n_estimators)\n",
    "    max_depth = int(max_depth)\n",
    "    min_samples_leaf = int(min_samples_leaf)\n",
    "    min_samples_split = int(min_samples_split)\n",
    "    model = RandomForestQuantileRegressor(random_state=0, \n",
    "                                          min_samples_split=min_samples_split,\n",
    "                                          n_estimators=n_estimators,\n",
    "                                          max_features = max_features,\n",
    "                                          max_depth=max_depth,\n",
    "                                          min_samples_leaf=min_samples_leaf,\n",
    "                                          n_jobs = -1)\n",
    "    return model\n",
    "\n",
    "# Function to create a DF with every combination of hyperparameters\n",
    "def expand_grid(data_dict):\n",
    "    rows = itertools.product(*data_dict.values())\n",
    "    return pd.DataFrame.from_records(rows, columns=data_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the cross-validation function that takes the potential parameters, X and y\n",
    "def timeseriesCVscore(params,X,y):\n",
    "\n",
    "    res = []\n",
    "    taus = np.arange(0.1,0.91,0.1)\n",
    "    my_tscv = TimeSeriesSplit(n_splits=4)\n",
    "\n",
    "    # The time series CV function takes the training data X and determines the splits\n",
    "    # so that we can train and test with a set of hyperparameters and evaluate the\n",
    "    # robustness of these hyperparameters.\n",
    "    for train_index, test_index in my_tscv.split(X):\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "        # Score using CRPS\n",
    "        model = qrf(params)\n",
    "        test_pred = np.vstack( [model.fit(X_train, y_train).predict(X_test, quantile = 100*q) for q in taus], ).T\n",
    "        res.append(CRPS(test_pred, y_test)[1])\n",
    "        \n",
    "    return(np.nanmean(res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This takes around 10-15 minutes so let this run in advance\n",
    "\n",
    "dat = pd.read_csv(\"/Users/Dennis/Desktop/SupplementaryMaterials/data/sxf_2015-2016.txt\", delimiter = \"\\t\",\n",
    "                 header='infer')\n",
    "dat['Time'] = pd.to_datetime(dat['Time']) # Make sure that pandas recognizes this as datetime\n",
    "dat = dat.set_index('Time') # Set time column as the index (easier for plotting)\n",
    "dat = dat.dropna(axis=0) # Remove the NaNs\n",
    "\n",
    "# Define the training and test set:\n",
    "X_train = dat.loc['2015-01-01':'2015-06-30', 'Ics':'G_nam'].reset_index().drop('Time', axis=1) # Because Time\n",
    "y_train = dat.loc['2015-01-01':'2016-06-30', 'dw_solar'].reset_index().drop('Time', axis=1)\n",
    "\n",
    "parameters_qrf = {\n",
    "                        \"min_samples_split\": [3],\n",
    "                        \"min_samples_leaf\": [1],\n",
    "                        \"max_features\": [0.75, 0.5],\n",
    "                        \"max_depth\":[8],\n",
    "                        \"n_estimators\":[500,1500]\n",
    "                 } \n",
    "\n",
    "parameters = expand_grid(parameters_qrf).values.tolist()\n",
    "parameters_qrf = expand_grid(parameters_qrf)\n",
    "    \n",
    "results = [] # Store the CRPS results, the order agrees with the order in the parameter grid\n",
    "for p in parameters:\n",
    "    res = timeseriesCVscore(p,X_train.to_numpy(),y_train.values.ravel()) # Note that we do not use the test data here.\n",
    "    results.append(res)\n",
    "                \n",
    "parameters_qrf['CRPS'] = pd.Series(results, index=parameters_qrf.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(parameters_qrf.sort_values(by=['CRPS'])) # Sort on CRPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(parameters_qrf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now that we have found the optimal hyperparameters, we can finally train and test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This takes around 25 minutes so do this beforehand\n",
    "taus = np.arange(0.1,0.91,0.1)\n",
    "# Define the training and test set:\n",
    "X_train = dat.loc['2015-01-01':'2016-01-31', 'Ics':'G_nam'].reset_index().drop('Time', axis=1) # Because Time\n",
    "y_train = dat.loc['2015-01-01':'2016-01-31', 'dw_solar'].reset_index().drop('Time', axis=1)\n",
    "\n",
    "X_test = dat.loc['2016-02-01':'2016-12-31', 'Ics':'G_nam'].reset_index().drop('Time', axis=1) # Because Time\n",
    "y_test = dat.loc['2016-02-01':'2016-12-31', 'dw_solar']\n",
    "\n",
    "params = parameters_qrf.iloc[0,:].drop(['CRPS']) # Take the best model parameters\n",
    "\n",
    "model_qrf = qrf(params) # Initiate the model with the optimal parameters\n",
    "y_hat_qrf = np.vstack( [model_qrf.fit(X_train.to_numpy(), y_train.values.ravel()).predict(X_test.to_numpy(), quantile = 100*q) for q in taus], ).T\n",
    "idx = y_test.to_numpy().nonzero()[0] # Check the non-zero values and disregard the rest\n",
    "\n",
    "print(\"CRPS =\",CRPS(y_hat_qrf[idx,:],y_test[idx])[1])\n",
    "print(\"Quantile score =\",np.mean([quantile_loss(q,y_test[idx],y_hat_qrf[idx,f]) for q,f in zip(taus,range(9))]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take the verification plots of the quantile regression model for comparison:\n",
    "myPlot(reliabilityDiagram(y_hat_qr[idx,:],y_test[idx]))\n",
    "rankHistogram(y_hat_qr[idx,:],y_test[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myPlot(reliabilityDiagram(y_hat_qrf[idx,:],y_test[idx]))\n",
    "rankHistogram(y_hat_qrf[idx,:],y_test[idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note how much we have been able to improve the bias of the forecasts. More elaborate cross-validation could easily improve this further but at the cost computational time.\n",
    "\n",
    "### Another helpful feature of decision tree type algorithms is the so-called variable importance. In short, due to its setup, it allows us to visually inspect the variables that were helpful in predicting (irradiance in this case) and random forests are therefore said to be robust to irrelevant variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code from: https://scikit-learn.org/stable/auto_examples/ensemble/plot_forest_importances.html\n",
    "\n",
    "importances = model_qrf.feature_importances_\n",
    "std = np.std([tree.feature_importances_ for tree in model_qrf.estimators_],\n",
    "             axis=0)\n",
    "indices = np.argsort(importances)[::-1]\n",
    "\n",
    "# Print the feature ranking\n",
    "print(\"Feature ranking:\")\n",
    "\n",
    "for f in range(X_train.shape[1]):\n",
    "    print(\"%d. feature %d (%f)\" % (f + 1, indices[f], importances[indices[f]]))\n",
    "\n",
    "# Plot the feature importances of the forest\n",
    "plt.figure()\n",
    "plt.title(\"Feature importances\")\n",
    "plt.bar(range(X_train.shape[1]), importances[indices],\n",
    "       color=\"r\", yerr=std[indices], align=\"center\")\n",
    "plt.xticks(range(X_train.shape[1]), indices)\n",
    "plt.xlim([-1, X_train.shape[1]])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A few words on benchmarks for probabilistic forecasts\n",
    "The probabilistic extension of the persistence benchmark $(\\hat{y}_{t+h} = y_t)$ would take the last $N$ observations and sort these to form a distribution. However, the choice of $N$ leads to different results and the persistence ensemble (PeEn) is therefore ambiguous because it has skill. The climatology is the most common benchmark in the atmospheric sciences and it is a forecast of the entire distribution of the dependent variable (e.g., irradiance, temperature, etc.).\n",
    "\n",
    "Recently, benchmarks have been proposed specifically for irradiance (Yang, 2019) and the clear-sky index (Munkhammar et al., 2019).\n",
    "\n",
    "The benchmark by Yang (complete history persistence ensemble, CH-PeEn) takes only time of day into consideration and forecasts the historical distribution of observations at that time of day.\n",
    "\n",
    "The benchmark by Munkhammar et al. (Markov-chain mixture model, MCM) is more advanced and estimates a transition matrix, which is conditioned with the latest observation, which then produces a piece-wise uniform distribution.\n",
    "\n",
    "More details can be found in the hand-outs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ingesting, cleaning and organizing data.\n",
    "\n",
    "The datasets we have used until now have been downloaded elsewhere. These data have either been cleaned by the people maintaining the sensor network or the scientists that published them online. However, we can also try to do this ourselves. After all, data ingestion and analysis is probably the most time consuming part of a machine learning task!  \n",
    "\n",
    "For this example, we'll look at atmospherical data from the AROME model, maintained by the Norwegian meteorological institute. This dataset has fewer dimensions than the one we are interested, namely the MetCoOp Ensemble Prediction System (MEPS), which is jointly run in operational routing by the Norwegian, Swedish and Finnish meteorological institutes. It consists of 10 ensemble members, has a horizontal resolution of 2.5 km, 65 vertical levels. MEPS is run 4 times a day at hourly resolution and produces forecasts up to 66 hours ahead.\n",
    "\n",
    "In the following, we first look at data from the AROME model to get an idea of the type of data we are dealing with and then continue with the MEPS forecasts. We'll download data for a few days and make some plots to see if what we observe makes sense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%html\n",
    "<iframe src=\"http://thredds.met.no/thredds/catalog/aromearcticlatest/catalog.html\" width=\"950\" height=\"500\"></iframe>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from netCDF4 import Dataset as NetCDFFile\n",
    "\n",
    "# Read in netcdf file, the original can be found here: \n",
    "# http://thredds.met.no/thredds/catalog/aromearcticlatest/catalog.html\n",
    "\n",
    "url ='http://thredds.met.no/thredds/dodsC/aromearcticlatest/arome_arctic_extracted_2_5km_latest.nc'\n",
    "nc = NetCDFFile(url)\n",
    "# Variables\n",
    "t2m = nc.variables['air_temperature_2m'][:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sys import getsizeof\n",
    "print(getsizeof(nc)) # in bytes\n",
    "print(getsizeof(t2m))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(nc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get specific coordinates from nc so that I know where to find UU:\n",
    "lat = nc.variables['latitude'][:] # (y,x) for some reason (latlon)\n",
    "np.shape(lat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get specific coordinates from nc so that I know where to find UU:\n",
    "lon = nc.variables['longitude'][:] # (y,x) for some reason\n",
    "np.shape(lon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "print(np.shape(t2m))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that returns the indices of the point closest to \"value\"\n",
    "def find_nearest(array, value):\n",
    "    X = np.abs(array - value)\n",
    "    idx = np.where( X == X.min() )\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I think the following is correct. Assuming latitude is y and longitude is x, \n",
    "# we find the y-coordinate of the latitude and the x-coordinate of the longitude.\n",
    "# I checked it with the latlon plots and t2m and it seems to be correct.\n",
    "\n",
    "# t2m at coordinate closest to 90,-30 for all forecast horizons (67 in total):\n",
    "np.asarray(t2m[:,0,find_nearest(lat, 90)[0],find_nearest(lon, -30)[1]][0:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "# Display matrix\n",
    "#plt.matshow(lat)\n",
    "#plt.matshow(lon)\n",
    "plt.matshow(t2m[0,0,:,:])\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now do the same but then with MEPS data, which has 10 ensemble members. Let's try to take a week's worth of data and store it in a matrix/dataframe. \n",
    "The variables of interest are cloud cover fraction [cloud_area_fraction(time,height0,ensemble_member,y,x)] and surface irradiance []. I don't exactly know which the latter is so let's stick to air temperature 2m above ground [air_temperature_2m(time,height1,ensemble_member,y,x)]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now with data that's of interest to me:\n",
    "\n",
    "from netCDF4 import Dataset as NetCDFFile\n",
    "# Read in netcdf file\n",
    "url = 'http://thredds.met.no/thredds/dodsC/meps25epsarchive/2018/06/01/meps_full_2_5km_20180601T00Z.nc'\n",
    "nc = NetCDFFile(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(nc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that returns the indices of the point closest to \"value\"\n",
    "def find_nearest(array, value):\n",
    "    X = np.abs(array - value)\n",
    "    idx = np.where( X == X.min() )\n",
    "    return idx\n",
    "\n",
    "# Extracting the coordinates only needs to be done once:\n",
    "from netCDF4 import Dataset as NetCDFFile\n",
    "url = 'http://thredds.met.no/thredds/dodsC/meps25epsarchive/2018/06/01/meps_extracted_2_5km_20180601T00Z.nc'\n",
    "nc = NetCDFFile(url)\n",
    "lat = nc.variables['latitude'][:] # (y,x) for some reason --> (latlon)\n",
    "lon = nc.variables['longitude'][:] # (y,x) for some reason"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's combine the above into a loop and cbind it into a list\n",
    "# The idea is to extract air temperature, cloud area fraction and time\n",
    "# and store these in lists. Afterwards, combine these into a DF.\n",
    "# Do this for M ensemble members, H forecast horizons and d days.\n",
    "\n",
    "from netCDF4 import Dataset as NetCDFFile\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime\n",
    "\n",
    "M = 5  # Ensemble members\n",
    "H = 24 # Forecast horizon\n",
    "\n",
    "lst_time = []\n",
    "lst_temp = []\n",
    "lst_tcc = []\n",
    "\n",
    "latitude = 59.858227\n",
    "longitude = 17.632252\n",
    "\n",
    "#days = [\"01\", \"02\", \"03\", \"04\", \"05\"]\n",
    "days = [\"01\", \"02\", \"03\", \"04\"]\n",
    "for d in days:\n",
    "    url = 'http://thredds.met.no/thredds/dodsC/meps25epsarchive/2018/06/' + d + '/meps_extracted_2_5km_201806' + d + 'T00Z.nc'\n",
    "    nc = NetCDFFile(url)\n",
    "    # Temperature:\n",
    "    t2m = nc.variables['air_temperature_2m'][0:H,0,0:M,find_nearest(lat, latitude)[0],find_nearest(lon, longitude)[1]]\n",
    "    temperature = np.reshape(np.asarray(t2m), (H,M))\n",
    "    lst_temp.append(temperature)\n",
    "    # Cloud cover:\n",
    "    tcc = nc.variables['cloud_area_fraction'][0:H,0,0:M,find_nearest(lat, latitude)[0],find_nearest(lon, longitude)[1]]\n",
    "    cloud = np.reshape(np.asarray(tcc), (H,M))\n",
    "    lst_tcc.append(cloud)\n",
    "    # Time:\n",
    "    time = nc.variables['time'][0:H]\n",
    "    tme = [pd.to_datetime(datetime.datetime.fromtimestamp(np.asarray(i)), utc=True) for i in time]\n",
    "    lst_time.append(tme)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mydf = pd.DataFrame(data=np.hstack((np.vstack(lst_temp),np.vstack(lst_tcc))), index = np.vstack(lst_time).flatten())\n",
    "mydf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alright, we have quite a few nonsensical values. Let's replace those with NaNs so we can interpolate them.\n",
    "tmp = mydf.where(mydf < 1000) # Replace values that are higher than 1000 with NaNs. Replace this with a reasonable limit for you variables.\n",
    "tmp.interpolate(method='linear', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Let's plot the results:\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "# Use seaborn style defaults and set the default figure size\n",
    "sns.set(rc={'figure.figsize':(11, 4)})\n",
    "cols_plot = [0, 1, 2, 3, 4] # Ensemble member 2 predicts a temperature of 9.96e+36 K, very uncommon in Sweden! ;) \n",
    "axes = tmp[cols_plot].plot(alpha=0.75, figsize=(11, 9), subplots=False)\n",
    "axes.set_ylabel('Temperature (K)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mydf.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
